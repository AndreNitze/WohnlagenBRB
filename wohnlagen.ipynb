{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58eeaf9ab328ebb",
   "metadata": {},
   "source": [
    "# Datenvorbereitung\n",
    "Zunächst werden für alle gewünschten Einflussfaktoren die Daten beschafft und in ein CSV-Format mit lat/lon oder GeoJSON-Geoemtry gebracht, sodass zu allen Adressen die gewünschten Eigenschaften vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "id": "d68fa405f11e996b",
   "metadata": {},
   "source": [
    "import helper\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from helper import load_geocsv, s\n",
    "from shapely.geometry import box\n",
    "\n",
    "EPSG_4326 = \"EPSG:4326\"\n",
    "\n",
    "# BBOX für Brandenburg an der Havel\n",
    "CITY_BOUNDING_BOX = box(12.3120236786, 52.2938432979, 12.7562682548, 52.5594777244)\n",
    "\n",
    "#  Stadtzentrum für Brandenburg an der Havel\n",
    "CITY_CENTER = (52.4116351153561, 12.556331280534392)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "87da6d87bee8556e",
   "metadata": {},
   "source": [
    "## Adressen einlesen\n",
    "Alle Adressen des Zielgebiets als CSV in einen GeoDataFrame einlesen."
   ]
  },
  {
   "cell_type": "code",
   "id": "9ade75ddfb40cfcf",
   "metadata": {},
   "source": "gdf_main = load_geocsv(\"out/adressen_mit_routen.csv\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef4f9c4ef5c70cf8",
   "metadata": {},
   "source": [
    "## Ortsteile der Stadt visualisieren und im Datensatz ergänzen"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc640d3dc01dd49c",
   "metadata": {},
   "source": [
    "import json\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "\n",
    "# Datei laden\n",
    "with open(\"data/ortsteile_brandenburg.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "# FeatureCollection extrahieren\n",
    "features = raw[\"features\"]\n",
    "gdf_ortsteile = gpd.GeoDataFrame.from_features(features)\n",
    "gdf_ortsteile.set_crs(EPSG_4326, inplace=True)\n",
    "gdf_ortsteile = gdf_ortsteile.clip(CITY_BOUNDING_BOX)\n",
    "\n",
    "# Karte zentrieren\n",
    "m = folium.Map(location=CITY_CENTER, zoom_start=12, tiles=\"cartodbpositron\")\n",
    "\n",
    "# Tooltip konfigurieren\n",
    "tooltip_fields = [\"otl_name\"]\n",
    "tooltip = folium.GeoJsonTooltip(\n",
    "    fields=tooltip_fields,\n",
    "    aliases=[\"Ortsteil\"],\n",
    "    localize=True,\n",
    "    sticky=True\n",
    ")\n",
    "\n",
    "# GeoJSON mit Tooltip zur Karte hinzufügen\n",
    "folium.GeoJson(\n",
    "    gdf_ortsteile,\n",
    "    name=\"Ortsteile\",\n",
    "    tooltip=tooltip\n",
    ").add_to(m)\n",
    "\n",
    "assert gdf_main.crs.to_epsg() == 4326\n",
    "assert gdf_ortsteile.crs.to_epsg() == 4326\n",
    "\n",
    "gdf_main = gpd.sjoin(gdf_main, gdf_ortsteile[[\"geometry\", \"otl_name\"]], how=\"left\", predicate=\"within\")\n",
    "gdf_main = gdf_main.rename(columns={\"otl_name\": \"ortsteil\"})\n",
    "gdf_main = gdf_main.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "\n",
    "# Drop rows with NaN lat/lon\n",
    "gdf_main = gdf_main.dropna(subset=[\"lat\"])\n",
    "\n",
    "# Mögliche Erweiterung: Ergänzung einer Spalte \"stadtteil\" für spätere Validierung und auch Visualisierung\n",
    "# Adressen im Zentrum haben korrekterweise keinen Ortsteil\n",
    "\n",
    "# Karte anzeigen\n",
    "m\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4a505c514a59316",
   "metadata": {},
   "source": [
    "## Explorative Datenanalyse"
   ]
  },
  {
   "cell_type": "code",
   "id": "ae54bd9392109709",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns           # nur für schönere Plots\n",
    "from helper import load_geocsv, make_merge_addr\n",
    "\n",
    "# Eindeutigen Merge-baren String aus Adresse erzeugen\n",
    "gdf_main[\"Adresse_merge\"] = gdf_main.apply(make_merge_addr, axis=1)\n",
    "\n",
    "gdf_main = gdf_main.rename(columns={\"geojson\": \"center_route\",\n",
    "                                    \"distance_m\" : \"center_distance\"})\n",
    "\n",
    "# Relevante Spalten auswählen\n",
    "gdf_main = gdf_main[[\"Straßenname\", \"Hsnr\", \"HsnrZus\",\n",
    "         \"center_distance\", \"lat\", \"lon\", \"geometry\", \"Adresse_merge\", \"ortsteil\", \"center_route\"]]\n",
    "\n",
    "# Histogramm der Distanzen zum Zentrum\n",
    "plt.figure(figsize=(6,3))\n",
    "sns.histplot(gdf_main[\"center_distance\"], bins=60, kde=False)\n",
    "plt.title(\"Verteilung der Fußentfernung zum Zentrum\")\n",
    "plt.xlabel(\"Distanz (Meter)\")\n",
    "plt.ylabel(\"Häufigkeit\")\n",
    "plt.show()\n",
    "print(gdf_main.shape)\n",
    "print(gdf_main.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c212e1e7f8401b2d",
   "metadata": {},
   "source": [
    "## Daten bereinigen"
   ]
  },
  {
   "cell_type": "code",
   "id": "d977cdd0e53252b4",
   "metadata": {},
   "source": [
    "print(gdf_main.shape)\n",
    "# Dubletten aus Adressen entfernen\n",
    "dups = gdf_main[\"Adresse_merge\"].value_counts()\n",
    "dups = dups[dups > 1]\n",
    "print(f\"{len(dups)} doppelte Adressen gefunden\")\n",
    "print(dups.head())\n",
    "\n",
    "# Ersten Treffer behalten\n",
    "gdf_main = gdf_main.sort_values(\"Adresse_merge\").drop_duplicates(\"Adresse_merge\", keep=\"first\")\n",
    "print(gdf_main.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "184ef02fc833f154",
   "metadata": {},
   "source": [
    "## Einzelhandel\n",
    "Separates Skript ```einzelhandel-adreessen.py``` ausfühen, um Datei \"adressen_mit_einzelhandel.csv\" zu erzeugen.\n",
    "Durch fußläufiges Routing berechnete Faktoren:\n",
    "- Anzahl von Einkaufsmöglichkeiten im Umkreis von 500 m\n",
    "- Anzahl von Einkaufsmöglichkeiten im Umkreis von 800 m\n",
    "- Geringste Distanz zum nächsten Einzelhandel"
   ]
  },
  {
   "cell_type": "code",
   "id": "f42b5b686e959068",
   "metadata": {},
   "source": [
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "\n",
    "# Lade vorberechnete Einzelhandel-Faktoren\n",
    "gdf_retail = load_geocsv(\"out/adressen_mit_einzelhandel.csv\")\n",
    "\n",
    "# Merge mit Haupt-GDF\n",
    "gdf_retail[\"Adresse_merge\"] = gdf_retail.apply(make_merge_addr, axis=1)\n",
    "gdf_retail = gdf_retail.sort_values(\"Adresse_merge\").drop_duplicates(\"Adresse_merge\", keep=\"first\")\n",
    "\n",
    "dups = gdf_retail[\"Adresse_merge\"].value_counts()\n",
    "print(dups[dups > 1])\n",
    "\n",
    "einzelhandel_attribute = [\"shop_min_m\", \"shops_500m_ct\", \"shops_800m_ct\"]\n",
    "\n",
    "gdf_main = pd.merge(\n",
    "    gdf_main,\n",
    "    gdf_retail[[\"Adresse_merge\"] + einzelhandel_attribute],\n",
    "    on=\"Adresse_merge\",\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_many\"\n",
    ")\n",
    "print(gdf_main.shape)\n",
    "print(gdf_main[[\"Adresse_merge\", \"shop_min_m\"]].head())\n",
    "\n",
    "print(gdf_main.shape)\n",
    "\n",
    "# Speicher freigeben\n",
    "del(gdf_retail)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40582ed950e765c9",
   "metadata": {},
   "source": [
    "# Verteilung Distanz zum nächsten Markt\n",
    "print(gdf_main.columns)\n",
    "sns.histplot(gdf_main[\"shop_min_m\"].dropna(), bins=40, kde=False)\n",
    "plt.title(\"Distanz zum nächsten Lebensmittel­markt\")\n",
    "plt.xlabel(\"Meter Fußweg\"); plt.ylabel(\"Adressen\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter Zentralität vs. Nahversorgung\n",
    "sns.scatterplot(x=\"center_distance\", y=\"shop_min_m\", data=gdf_main, alpha=.3)\n",
    "plt.xlabel(\"Distanz Zentrum (m)\"); plt.ylabel(\"Distanz nächster Markt (m)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2f323afbef7d8d4",
   "metadata": {},
   "source": [
    "## Lärmbelastung\n",
    "Siehe ```laerm.ipynb``` zur Erzeugung, ansonsten einfach ```data/adressen_mit_laerm.csv``` verwenden und per Spalte 'geometry' verschneiden.\n",
    "\n",
    "Definition Lärm-Index:\n",
    "- NaN / leer: kein gemessener Straßenlärm\n",
    "- 0: 55 - 59 dB\n",
    "- 1: 60 - 64 dB\n",
    "- 2: 65 - 69 dB\n",
    "- 3: 70 - 74 dB\n",
    "- 4: >= 75 db"
   ]
  },
  {
   "cell_type": "code",
   "id": "68ddd77c43b68583",
   "metadata": {},
   "source": [
    "gdf_laerm_karte = load_geocsv(\"out/adressen_mit_laerm.csv\")\n",
    "print(gdf_laerm_karte.shape)\n",
    "print(gdf_main.shape)\n",
    "\n",
    "# Merge des Lärmindex mit Haupt-GDF\n",
    "gdf_laerm_karte[\"Adresse_merge\"] = gdf_laerm_karte.apply(make_merge_addr, axis=1)\n",
    "gdf_laerm_karte[\"Laerm_index_tag\"] = gdf_laerm_karte[\"Laerm_index_tag\"].fillna(-0.1)\n",
    "\n",
    "# Deduplicate, im Zweifel nimm den lautesten Wert\n",
    "gdf_laerm_karte = (\n",
    "    gdf_laerm_karte\n",
    "    .sort_values(\"Laerm_index_tag\", ascending=False)\n",
    "    .drop_duplicates(\"Adresse_merge\", keep=\"first\")\n",
    ")\n",
    "\n",
    "laerm_attribute = [\"Laerm_index_tag\"]\n",
    "gdf_main = pd.merge(\n",
    "    gdf_main,\n",
    "    gdf_laerm_karte[[\"Adresse_merge\"] + laerm_attribute],\n",
    "    on=\"Adresse_merge\",\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_many\"\n",
    ")\n",
    "\n",
    "#print(gdf_main.columns)\n",
    "print(gdf_main.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12690bd38c8a865e",
   "metadata": {},
   "source": [
    "## Bildung\n",
    "Weitere Faktoren\n",
    "- Fußläufige Entfernung zur nächstgelegenen Kita\n",
    "- Fußläufige Entfernung zur nächstgelegenen Schule\n",
    "- Anzahl Kitas im Umkreis von 500, 800 und 1000 Metern\n",
    "- Anzahl Schulen im Umkreis von 500, 800 und 1000 Metern"
   ]
  },
  {
   "cell_type": "code",
   "id": "1194b88750466342",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "\n",
    "# Daten als GeoDataFrame einlesen\n",
    "gdf_kitas = load_geocsv(\"out/adressen_mit_kita_routen.csv\")\n",
    "gdf_grundschulen = load_geocsv(\"out/adressen_mit_grundschul_routen.csv\")\n",
    "print(gdf_kitas.shape)\n",
    "print(gdf_grundschulen.shape)\n",
    "print(gdf_main.shape)\n",
    "\n",
    "# Eindeutige Adresse für den Merge generieren\n",
    "gdf_kitas[\"Adresse_merge\"] = gdf_kitas.apply(make_merge_addr, axis=1)\n",
    "gdf_kitas = gdf_kitas.drop_duplicates(\"Adresse_merge\")\n",
    "gdf_grundschulen[\"Adresse_merge\"] = gdf_grundschulen.apply(make_merge_addr, axis=1)\n",
    "gdf_grundschulen = gdf_grundschulen.drop_duplicates(\"Adresse_merge\")\n",
    "\n",
    "kitas_attribute = [\"kitas_min_distance_m\", \"kitas_geometry\", \"kitas_count_within_500m\", \"kitas_count_within_800m\", \"kitas_count_within_1000m\"]\n",
    "gdf_main = pd.merge(\n",
    "    gdf_main,\n",
    "    gdf_kitas[[\"Adresse_merge\"] + kitas_attribute],\n",
    "    on=\"Adresse_merge\",\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_many\"\n",
    ")\n",
    "\n",
    "grundschulen_attribute = [\"grundschulen_min_distance_m\", \"grundschulen_geometry\", \"grundschulen_count_within_500m\", \"grundschulen_count_within_800m\", \"grundschulen_count_within_1000m\"]\n",
    "gdf_main = pd.merge(\n",
    "    gdf_main,\n",
    "    gdf_grundschulen[[\"Adresse_merge\"] + grundschulen_attribute],\n",
    "    on=\"Adresse_merge\",\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_many\"\n",
    ")\n",
    "del(gdf_kitas, gdf_grundschulen)  # Speicher freigeben\n",
    "\n",
    "print(gdf_main.columns)\n",
    "print(gdf_main.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6461cab96260ab5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Medizinische Versorgung\n",
    "Ein \"medizinisches Zentrum\" wurde definiert als eine Apotheke mit zwei Ärzten im Umkreis von 100 Metern. Diese Zentren werden separat in ```medizinische-zentren.py``` berechnet. Dazu wird die euklidische Distanz (\"Luftlinie\") zwischen Apotheken und umgebenden Ärzten berechnet.\n",
    "Dadurch entstehen für den Datensatz folgende neue Attribute:\n",
    "- Anzahl von medizinischen Zentren im Umkreis von 500 m\n",
    "- Anzahl von medizinischen Zentren im Umkreis von 800 m\n",
    "- Anzahl von medizinischen Zentren im Umkreis von 1000 m\n",
    "- Fußläufige Distanz zum nächsten medizinischen Zentrum\n",
    "\n",
    "Die vollständige Erklärung der Felder findet sich im Anhang.\n",
    "\n",
    "Die fußläufige Distanz zum nächstgelegenen medizinischen Zentrum wird per ```routing.py``` für jede Adresse einzeln ermittelt und als Weg gespeichert (```adressen_mit_medzentren_routen```)."
   ],
   "id": "4bc6829d0589c7ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Zentren / Apothekenstandorte mit Klassifikation\n",
    "df_centers = pd.read_csv(\"out/medzentren_geocoded.csv\")\n",
    "\n",
    "# Normiere Bool-Spalte\n",
    "df_centers[\"is_med_center\"] = (df_centers[\"is_med_center\"].astype(str).str.lower().isin([\"true\", \"1\", \"yes\", \"y\"]))\n",
    "\n",
    "# Einzel-Apotheken ausblenden?\n",
    "# df_centers = df_centers[df_centers[\"is_med_center\"] == True].copy()\n",
    "\n",
    "def build_popup(row):\n",
    "    lines = []\n",
    "\n",
    "    if bool(row[\"is_med_center\"]):\n",
    "        lines.append(f\"<b>Medizinisches Zentrum \\\"{str(row['Strassenname'])}\\\"</b><br>\")\n",
    "\n",
    "    # Name\n",
    "    if pd.notna(row.get(\"Name_Apotheke\")) and str(row[\"Name_Apotheke\"]).strip() != \"\":\n",
    "        lines.append(str(row[\"Name_Apotheke\"]))\n",
    "\n",
    "    # Ärztedichte im 100m Radius\n",
    "    if \"arzt_ct_100m\" in row:\n",
    "        lines.append(f\"<br><b>{int(row['arzt_ct_100m'])}</b> Arzt-Praxen\")\n",
    "\n",
    "    return \"<br>\".join(lines)\n",
    "\n",
    "def pick_icon(row):\n",
    "    if bool(row[\"is_med_center\"]):\n",
    "        return folium.Icon(color=\"green\", icon=\"plus-sign\", prefix=\"glyphicon\")\n",
    "    else:\n",
    "        return folium.Icon(color=\"gray\", icon=\"info-sign\", prefix=\"glyphicon\")\n",
    "\n",
    "m = folium.Map(location=CITY_CENTER, zoom_start=14)\n",
    "cluster = MarkerCluster(name=\"Medizinische Zentren / Apotheken\")\n",
    "cluster.add_to(m)\n",
    "\n",
    "# Marker setzen\n",
    "for _, row in df_centers.iterrows():\n",
    "    lat = row[\"lat\"]\n",
    "    lon = row[\"lon\"]\n",
    "\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        continue\n",
    "\n",
    "    popup_html = build_popup(row)\n",
    "    icon = pick_icon(row)\n",
    "\n",
    "    folium.Marker(\n",
    "        location=[lat, lon],\n",
    "        popup=popup_html,\n",
    "        tooltip=row.get(\"Strassenname\", \"Apotheke\"),\n",
    "        icon=icon,\n",
    "    ).add_to(cluster)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ],
   "id": "afd5ce10da8085a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Medizinische Felder in gdf_main übernehmen\n",
    "# 1. Routing-Ergebnis für medizinische Versorgung laden\n",
    "gdf_med = load_geocsv(\"out/adressen_mit_medzentren_routen.csv\")\n",
    "print(\"gdf_med shape:\", gdf_med.shape)\n",
    "print(\"gdf_main shape (vor Merge):\", gdf_main.shape)\n",
    "\n",
    "# 2. Merge-Key bauen (Adresse normalisieren)\n",
    "gdf_med[\"Adresse_merge\"] = gdf_med.apply(make_merge_addr, axis=1)\n",
    "gdf_med = gdf_med.drop_duplicates(\"Adresse_merge\")\n",
    "\n",
    "# 3. Relevante Attributspalten aus der medizinischen Versorgung definieren\n",
    "medzentren_attribute = [\n",
    "    \"medzentren_min_distance_m\",\n",
    "    \"medzentren_geometry\",\n",
    "    \"medzentren_count_within_500m\",\n",
    "    \"medzentren_count_within_800m\",\n",
    "    \"medzentren_count_within_1000m\",\n",
    "]\n",
    "\n",
    "# 4. Merge in Haupt-GDF\n",
    "gdf_main = pd.merge(\n",
    "    gdf_main,\n",
    "    gdf_med[[\"Adresse_merge\"] + medzentren_attribute],\n",
    "    on=\"Adresse_merge\",\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_one\",\n",
    ")\n",
    "\n",
    "print(\"gdf_main shape (nach Merge):\", gdf_main.shape)"
   ],
   "id": "3b857325d707d36a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc8d68c7ce48ddaa",
   "metadata": {},
   "source": [
    "## ÖPNV-Qualität\n",
    "\n",
    "Die Qualität des ÖPNV wird anhand der Fußläufigkeit zur nächsten Haltestelle und der Häufigkeit von Abfahrten (Headway) bewertet. Die Daten stammen vom Verkehrsverbund Berlin-Brandenburg (VBB), Lizenz: CC BY 4.0, [zu den Daten](https://unternehmen.vbb.de/digitale-services/datensaetze).\n",
    "\n",
    "Vorausgesetzte Datensätze:\n",
    "- ```data/GTFS/stops.txt```, der die Haltestellen mit ihren Geokoordinaten enthält.\n",
    "- ```data/adressen_mit_haltestellen_routen.csv```, der die Routen zu den Haltestellen und Anzahl von Haltestellen im Radius 500m und 800m enthält.\n",
    "\n",
    "Probleme mit Datenqualität:\n",
    "- In der \"2024_Haltestellen.csv\" fehlen die Haltestellen \"Libellenweg\" und \"Immenweg\" (Linie B). Dadurch werden diese Haltestellen nicht berücksichtigt und Wege zur nächsten Haltestelle werden länger eingeschätzt."
   ]
  },
  {
   "cell_type": "code",
   "id": "e307190bd56921b7",
   "metadata": {},
   "source": [
    "# Load GTFS stops data\n",
    "stops = pd.read_csv(\"data/GTFS/stops.txt\")\n",
    "stops[\"geometry\"] = stops.apply(lambda row: Point(row[\"stop_lon\"], row[\"stop_lat\"]), axis=1)\n",
    "gdf_stops = gpd.GeoDataFrame(stops, geometry=\"geometry\", crs=EPSG_4326)\n",
    "\n",
    "# Convert to UTM for precise metric distance calculations\n",
    "gdf_main = gdf_main.to_crs(epsg=32633)\n",
    "gdf_stops = gdf_stops.to_crs(epsg=32633)\n",
    "\n",
    "# Assign nearest stop to each address based on Point(X, Y) geometry\n",
    "gdf_main[\"nearest_stop_id\"] = gdf_main.geometry.apply(\n",
    "    lambda pt: gdf_stops.loc[gdf_stops.distance(pt).idxmin(), \"stop_id\"]\n",
    ")\n",
    "\n",
    "# Read routes from adresses to stops and stop count from prepared CSV\n",
    "gdf_haltestellen = load_geocsv(\"out/adressen_mit_haltestellen_routen.csv\")\n",
    "gdf_haltestellen[\"Adresse_merge\"] = gdf_haltestellen.apply(make_merge_addr, axis=1)\n",
    "gdf_haltestellen = gdf_haltestellen.drop_duplicates(\"Adresse_merge\").copy()\n",
    "\n",
    "print(\"gdf_haltestellen shape:\", gdf_haltestellen.shape)\n",
    "print(\"gdf_main shape (vor Merge):\", gdf_main.shape)\n",
    "\n",
    "# Merge haltestellen attributes into main GeoDataFrame\n",
    "haltestellen_attribute = [\"haltestellen_geometry\", \"haltestellen_min_distance_m\", \"haltestellen_count_within_500m\", \"haltestellen_count_within_800m\"]\n",
    "gdf_main = pd.merge(\n",
    "    gdf_main,\n",
    "    gdf_haltestellen[[\"Adresse_merge\"] + haltestellen_attribute],\n",
    "    on=\"Adresse_merge\",\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_one\"\n",
    ")\n",
    "\n",
    "print(\"gdf_main shape (nach Merge):\", gdf_main.shape)\n",
    "\n",
    "# Speicher freigeben\n",
    "del(gdf_haltestellen)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bb890f9e9a5f3e2c",
   "metadata": {},
   "source": [
    "### Berechnung der ÖPNV-Taktung"
   ]
  },
  {
   "cell_type": "code",
   "id": "a6f772eabddaacc9",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. GTFS laden\n",
    "# -------------------------------------------------\n",
    "stops = pd.read_csv(\"data/GTFS/stops.txt\")\n",
    "stop_times = pd.read_csv(\"data/GTFS/stop_times.txt\", low_memory=False)\n",
    "trips = pd.read_csv(\"data/GTFS/trips.txt\")\n",
    "calendar = pd.read_csv(\"data/GTFS/calendar.txt\")  # behalten wir für evtl. spätere Filter, aber nicht hart\n",
    "# optional, falls vorhanden:\n",
    "# calendar_dates = pd.read_csv(\"data/GTFS/calendar_dates.txt\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Zeitspalte -> Minuten ab Mitternacht\n",
    "# -------------------------------------------------\n",
    "def parse_time_to_minutes(t):\n",
    "    if pd.isna(t):\n",
    "        return None\n",
    "    parts = str(t).split(\":\")\n",
    "    if len(parts) == 2:\n",
    "        h, m = parts\n",
    "    elif len(parts) == 3:\n",
    "        h, m, _s = parts\n",
    "    else:\n",
    "        return None\n",
    "    try:\n",
    "        h = int(h)\n",
    "        m = int(m)\n",
    "        return h * 60 + m\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "stop_times = stop_times.copy()\n",
    "stop_times[\"minutes\"] = stop_times[\"arrival_time\"].apply(parse_time_to_minutes)\n",
    "stop_times = stop_times.dropna(subset=[\"minutes\"])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Wir interessieren uns für HVZ-Fenster\n",
    "#    Wir extrahieren NUR Stop-Zeiten, die überhaupt in diesen Fenstern liegen.\n",
    "#    Damit behalten wir reale Pendlerfahrten selbst dann,\n",
    "#    wenn calendar.monday == 0 gesagt hätte.\n",
    "# -------------------------------------------------\n",
    "\n",
    "HVZ_WINDOWS = [\n",
    "    (360, 540),   # 06:00–09:00\n",
    "    (960, 1140),  # 16:00–19:00\n",
    "]\n",
    "\n",
    "def in_any_window(mins, windows):\n",
    "    for lo, hi in windows:\n",
    "        if lo <= mins <= hi:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "stop_times_hvz = stop_times[stop_times[\"minutes\"].apply(lambda mm: in_any_window(mm, HVZ_WINDOWS))].copy()\n",
    "\n",
    "# 3. Filter nur auf werktägliche Dienste\n",
    "trips_filtered = trips.merge(calendar, on=\"service_id\")\n",
    "trips_filtered = trips_filtered[trips_filtered[\"monday\"] == 1]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. stop_times_hvz ↔ trips (route_id etc.)\n",
    "# -------------------------------------------------\n",
    "trips[\"trip_id\"] = trips[\"trip_id\"].astype(str)\n",
    "\n",
    "stop_times_hvz[\"trip_id\"] = stop_times_hvz[\"trip_id\"].astype(str)\n",
    "\n",
    "stopdata = stop_times_hvz.merge(\n",
    "    trips[[\"trip_id\", \"route_id\", \"service_id\"]],\n",
    "    on=\"trip_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Haltestellengeometrie + Clusterbildung (DBSCAN)\n",
    "# -------------------------------------------------\n",
    "stops = stops.copy()\n",
    "stops[\"stop_id\"] = stops[\"stop_id\"].astype(str)\n",
    "\n",
    "gdf_stops_all = gpd.GeoDataFrame(\n",
    "    stops,\n",
    "    geometry=gpd.points_from_xy(stops[\"stop_lon\"], stops[\"stop_lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "gdf_stops_metric = gdf_stops_all.to_crs(epsg=25833).copy()\n",
    "coords = np.vstack([\n",
    "    gdf_stops_metric.geometry.x.values,\n",
    "    gdf_stops_metric.geometry.y.values\n",
    "]).T\n",
    "\n",
    "db = DBSCAN(eps=100, min_samples=1).fit(coords)\n",
    "gdf_stops_all[\"pt_cluster_id\"] = db.labels_.astype(int)\n",
    "\n",
    "stop_to_cluster = dict(zip(gdf_stops_all[\"stop_id\"], gdf_stops_all[\"pt_cluster_id\"]))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Cluster-ID an stopdata hängen\n",
    "# -------------------------------------------------\n",
    "stopdata[\"stop_id\"] = stopdata[\"stop_id\"].astype(str)\n",
    "stopdata[\"pt_cluster_id\"] = stopdata[\"stop_id\"].map(stop_to_cluster)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. Headway je Cluster berechnen\n",
    "#    jetzt auf Basis ALLER HVZ-Fahrten, die tatsächlich vorkommen,\n",
    "#    statt nur calendar.monday==1\n",
    "# -------------------------------------------------\n",
    "def compute_headway_for_window(df, lo, hi, group_col=\"pt_cluster_id\", time_col=\"minutes\"):\n",
    "    result = {}\n",
    "    for clus, group in df.groupby(group_col):\n",
    "        if pd.isna(clus):\n",
    "            continue\n",
    "        # nur Zeiten im Fenster\n",
    "        times = sorted([t for t in group[time_col] if lo <= t <= hi])\n",
    "        if len(times) < 2:\n",
    "            continue\n",
    "        diffs = [b - a for a, b in zip(times, times[1:])]\n",
    "        result[clus] = sum(diffs) / len(diffs)\n",
    "    return result\n",
    "\n",
    "headway_morning = compute_headway_for_window(stopdata, 360, 540)\n",
    "headway_evening = compute_headway_for_window(stopdata, 960, 1140)\n",
    "\n",
    "df_hm = (\n",
    "    pd.DataFrame.from_dict(headway_morning, orient=\"index\", columns=[\"headway_morning\"])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"pt_cluster_id\"})\n",
    ")\n",
    "df_he = (\n",
    "    pd.DataFrame.from_dict(headway_evening, orient=\"index\", columns=[\"headway_evening\"])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"pt_cluster_id\"})\n",
    ")\n",
    "\n",
    "df_headways = df_hm.merge(df_he, on=\"pt_cluster_id\", how=\"outer\", validate=\"one_to_one\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8. Merge in gdf_main\n",
    "# -------------------------------------------------\n",
    "gdf_main[\"nearest_stop_id\"] = gdf_main[\"nearest_stop_id\"].astype(str)\n",
    "gdf_main[\"pt_cluster_id\"] = gdf_main[\"nearest_stop_id\"].map(stop_to_cluster)\n",
    "\n",
    "# weg mit alten Spalten falls rerun\n",
    "for col in [\n",
    "    \"headway_morning\", \"headway_evening\", \"headway_avg\",\n",
    "    \"headway_morning_score_fixed\", \"headway_evening_score_fixed\",\n",
    "    \"headway_avg_score_fixed\",\n",
    "]:\n",
    "    if col in gdf_main.columns:\n",
    "        gdf_main = gdf_main.drop(columns=[col])\n",
    "\n",
    "gdf_main = gdf_main.merge(\n",
    "    df_headways,\n",
    "    on=\"pt_cluster_id\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9. Kennzahlen\n",
    "# -------------------------------------------------\n",
    "for col in [\"headway_morning\", \"headway_evening\"]:\n",
    "    gdf_main[col] = pd.to_numeric(gdf_main[col], errors=\"coerce\")\n",
    "\n",
    "gdf_main[\"headway_avg\"] = gdf_main[[\"headway_morning\", \"headway_evening\"]].mean(axis=1)\n",
    "\n",
    "fixed_min, fixed_max = 5, 60\n",
    "def scoreify(series):\n",
    "    return 1 - ((series - fixed_min) / (fixed_max - fixed_min)).clip(lower=0, upper=1)\n",
    "\n",
    "gdf_main[\"headway_morning_score_fixed\"] = scoreify(gdf_main[\"headway_morning\"])\n",
    "gdf_main[\"headway_evening_score_fixed\"] = scoreify(gdf_main[\"headway_evening\"])\n",
    "gdf_main[\"headway_avg_score_fixed\"]     = scoreify(gdf_main[\"headway_avg\"])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 10. Debug neu\n",
    "# -------------------------------------------------\n",
    "no_headway_mask = gdf_main[\"headway_avg\"].isna()\n",
    "\n",
    "print(\"Adressen ohne Headway_avg:\", int(no_headway_mask.sum()))\n",
    "\n",
    "na_clusters = (\n",
    "    gdf_main.loc[no_headway_mask, \"pt_cluster_id\"]\n",
    "    .value_counts()\n",
    ")\n",
    "print(\"Problem-Cluster:\", len(na_clusters))\n",
    "print(na_clusters.head(20))\n",
    "\n",
    "# sicherstellen, dass wir wirklich noch Stops ohne Window-Fahrten haben\n",
    "clusters_with_headway = set(df_headways[\"pt_cluster_id\"].tolist())\n",
    "clusters_without = [cid for cid in gdf_main[\"pt_cluster_id\"].dropna().unique()\n",
    "                    if cid not in clusters_with_headway]\n",
    "print(\"Cluster ohne Eintrag in df_headways:\", len(clusters_without))\n",
    "print(clusters_without[:20])\n",
    "\n",
    "# Spot-check: nimm den größten Problem-Cluster\n",
    "if len(clusters_without) > 0:\n",
    "    test_cluster = clusters_without[0]\n",
    "    sample_times = stopdata.loc[stopdata[\"pt_cluster_id\"] == test_cluster, \"minutes\"]\n",
    "    print(\"Beispiel-Cluster\", test_cluster, \"HVZ Zeiten:\",\n",
    "          sorted([t for t in sample_times if 360 <= t <= 540])[:20])\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11. Debug: Wo fehlen noch Headways?\n",
    "# -------------------------------------------------\n",
    "no_headway_mask = gdf_main[\"headway_avg\"].isna()\n",
    "\n",
    "print(\"Adressen ohne Headway_avg:\", int(no_headway_mask.sum()))\n",
    "\n",
    "# Welche Stop-IDs sind schuld, nach nearest_stop_id\n",
    "na_stops = (\n",
    "    gdf_main.loc[no_headway_mask, \"nearest_stop_id\"]\n",
    "    .value_counts()\n",
    ")\n",
    "print(\"Anzahl unterschiedlicher problematischer Haltestellen (Steig-Ebene):\", len(na_stops))\n",
    "print(na_stops.head(20))\n",
    "\n",
    "# Welche Cluster sind schuld\n",
    "na_clusters = (\n",
    "    gdf_main.loc[no_headway_mask, \"pt_cluster_id\"]\n",
    "    .value_counts()\n",
    ")\n",
    "print(\"Anzahl problematischer Cluster:\", len(na_clusters))\n",
    "print(na_clusters.head(20))\n",
    "\n",
    "# Prüfen, ob diese Cluster überhaupt Headways in df_headways haben\n",
    "clusters_with_headway = set(df_headways[\"pt_cluster_id\"].astype(int).tolist())\n",
    "clusters_without = [cid for cid in na_clusters.index if cid not in clusters_with_headway]\n",
    "print(\"Cluster ohne Eintrag in df_headways:\", len(clusters_without))\n",
    "print(clusters_without[:20])\n",
    "\n",
    "# Sanity check: für einen der Top-Cluster, zeig alle Abfahrtszeiten morgens\n",
    "if len(clusters_without) > 0:\n",
    "    test_cluster = clusters_without[0]\n",
    "    sample_times = stopdata.loc[stopdata[\"pt_cluster_id\"] == test_cluster, \"minutes\"]\n",
    "    print(\"Beispiel-Cluster\", test_cluster, \"hat morgens Zeiten in 6-9?:\",\n",
    "          sample_times[(sample_times>=360)&(sample_times<=540)].sort_values().head(20).tolist())\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 12. Cleanup (optional)\n",
    "# -------------------------------------------------\n",
    "#del(stop_times, trips, calendar, trips_filtered,\n",
    "#    stopdata, df_hm, df_he, df_headways,\n",
    "#    gdf_stops_metric, coords, db)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c392704b502cf76f",
   "metadata": {},
   "source": [
    "## Visualisierung der ÖPNV-Taktung"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c4d67f47fbd596",
   "metadata": {},
   "source": [
    "import folium\n",
    "from branca.colormap import linear, LinearColormap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert gdf_stops geometry to WGS84\n",
    "gdf_stops = gdf_stops.to_crs(epsg=4326).copy()\n",
    "\n",
    "# Haltestellen innerhalb Stadt\n",
    "gdf_stops_clip = gdf_stops[gdf_stops.geometry.within(CITY_BOUNDING_BOX)].copy()\n",
    "print(\"Haltestellen im Stadtpolygon:\", len(gdf_stops_clip))\n",
    "\n",
    "#\n",
    "# 2. Farbskala nur aus Adressen-Headway\n",
    "#\n",
    "# headway_avg = durchschnittliche Taktzeit (Minuten) für diese Adresse\n",
    "# Annahme: kleiner Wert = besser (häufigere Bedienung)\n",
    "addr_headway = pd.to_numeric(gdf_main[\"headway_avg\"], errors=\"coerce\")\n",
    "\n",
    "hv_valid = addr_headway.dropna()\n",
    "if len(hv_valid) > 0:\n",
    "    vmin, vmax = hv_valid.quantile([0.01, 0.99])\n",
    "    if vmin == vmax:\n",
    "        # falls alles gleich (z. B. nur eine Linie), spreizen für die Farbskala\n",
    "        vmin = vmin - 0.1\n",
    "        vmax = vmax + 0.1\n",
    "else:\n",
    "    # Fallback, falls ALLE Adressen NaN sind\n",
    "    vmin, vmax = (0, 1)\n",
    "\n",
    "palette_normal = list(linear.RdYlGn_11.colors)\n",
    "palette_inverted = palette_normal[::-1]\n",
    "colormap = LinearColormap(\n",
    "    colors=palette_inverted,\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    ").to_step(n=9)\n",
    "colormap.caption = \"Headway pro Adresse (Minuten, kleiner = besser)\"\n",
    "\n",
    "# 3. Karte initialisieren\n",
    "m = folium.Map(location=CITY_CENTER, zoom_start=13, tiles=\"cartodbpositron\")\n",
    "\n",
    "# 4. Adressen plotten (farbig nach headway_avg)\n",
    "#    - Farbig wenn headway_avg da\n",
    "#    - Hellgrau wenn kein Wert\n",
    "for _, row in gdf_main.iterrows():\n",
    "    hv_addr = row.get(\"headway_avg\", np.nan)\n",
    "    hv_morning = row.get(\"headway_morning\", np.nan)\n",
    "    hv_evening = row.get(\"headway_evening\", np.nan)\n",
    "\n",
    "    if pd.isna(hv_addr):\n",
    "        # Kein Wert berechnet -> zeichne neutral\n",
    "        color = \"#BBBBBB\"\n",
    "        fill_color = \"#BBBBBB\"\n",
    "        hv_label = \"kein Wert\"\n",
    "    else:\n",
    "        color = colormap(hv_addr)\n",
    "        fill_color = colormap(hv_addr)\n",
    "        hv_label = f\"{hv_addr:.1f} min\"\n",
    "\n",
    "    # Popup mit allen Headways, falls vorhanden\n",
    "    popup_lines = [\n",
    "        f\"{row.get('Straßenname', '')} {row.get('Hsnr', '')}\",\n",
    "        f\"<b>Headway (avg):</b> {hv_label}\",\n",
    "    ]\n",
    "    if pd.notna(hv_morning):\n",
    "        popup_lines.append(f\"Frühspitze: {hv_morning:.1f} min\")\n",
    "    if pd.notna(hv_evening):\n",
    "        popup_lines.append(f\"Abendspitze: {hv_evening:.1f} min\")\n",
    "\n",
    "    popup_html = \"<br>\".join(popup_lines)\n",
    "\n",
    "    folium.CircleMarker(\n",
    "        location=[row.lat, row.lon],\n",
    "        radius=5,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=fill_color,\n",
    "        fill_opacity=0.8,\n",
    "        weight=1,\n",
    "        popup=popup_html,\n",
    "    ).add_to(m)\n",
    "\n",
    "# 5. Haltestellen plotten (schwarz, neutral)\n",
    "#    Du zeigst hier die Infrastrukturpunkte, ohne Qualitätsfarbe.\n",
    "for _, row in gdf_stops_clip.iterrows():\n",
    "    lat_s = row[\"stop_lat\"]\n",
    "    lon_s = row[\"stop_lon\"]\n",
    "\n",
    "    # Popup Haltestellenname\n",
    "    stop_label = row.get(\"stop_name\", row.get(\"stop_id\", \"Haltestelle\"))\n",
    "\n",
    "    folium.CircleMarker(\n",
    "        location=[lat_s, lon_s],\n",
    "        radius=3,\n",
    "        color=\"black\",\n",
    "        fill=True,\n",
    "        fill_color=\"black\",\n",
    "        fill_opacity=1,\n",
    "        weight=1,\n",
    "        popup=f\"<b>Haltestelle:</b> {stop_label}<br>\",\n",
    "    ).add_to(m)\n",
    "\n",
    "# 6. Legende für die Adressen-Headways\n",
    "colormap.add_to(m)\n",
    "m\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "868139354b85b686",
   "metadata": {},
   "source": [
    "# Visualisierung aller Indikatoren\n",
    "Alle Einflussfaktoren (Supermärkte, Ärzte, Schulen etc.) werden zur Plausibilitätsprüfung auf einer Karte visualisiert."
   ]
  },
  {
   "cell_type": "code",
   "id": "420b28d5f45c67fb",
   "metadata": {},
   "source": [
    "import folium\n",
    "import branca.colormap as cm\n",
    "from helper import add_markers_from_csv, STRASSENNAME, HAUSNUMMER, HAUSNUMMERZUSATZ\n",
    "\n",
    "m = folium.Map(location=CITY_CENTER, zoom_start=13, tiles=\"cartodbpositron\")\n",
    "\n",
    "# Add layer markers\n",
    "add_markers_from_csv(map_obj=m, csv_path=\"out/einzelhandel_geocoded.csv\", color=\"blue\", icon=\"shopping-cart\", layer_name=\"Einzelhandel\")\n",
    "add_markers_from_csv(map_obj=m, csv_path=\"out/grundschulen_geocoded.csv\", color=\"green\", icon=\"graduation-cap\", layer_name=\"Grundschulen\")\n",
    "add_markers_from_csv(map_obj=m, csv_path=\"out/kitas_geocoded.csv\", color=\"beige\", icon=\"child\", layer_name=\"Kitas\")\n",
    "add_markers_from_csv(map_obj=m, csv_path=\"out/haltestellen_geocoded.csv\", color=\"lightgray\", icon=\"bus\", layer_name=\"Haltestellen\")\n",
    "add_markers_from_csv(map_obj=m, csv_path=\"out/apotheken_geocoded.csv\", color=\"red\", icon=\"staff-snake\", layer_name=\"Apotheken\")\n",
    "add_markers_from_csv(map_obj=m, csv_path=\"out/aerzte_geocoded.csv\", color=\"lightred\", icon=\"hospital\", layer_name=\"Ärzte\")\n",
    "\n",
    "# Lärmindex aus dem Geopackage\n",
    "gdf_laerm_karte = gpd.read_file(\"data/laerm.gpkg\")\n",
    "gdf_laerm_karte = gdf_laerm_karte.clip(CITY_BOUNDING_BOX)\n",
    "value_column = \"isov1\"\n",
    "min_val = gdf_laerm_karte[value_column].min()\n",
    "max_val = gdf_laerm_karte[value_column].max()\n",
    "colormap = cm.LinearColormap(colors=[\"green\", \"yellow\", \"red\"], vmin=min_val, vmax=max_val)\n",
    "colormap.caption = \"Lärmpegel (LDEN in dB)\"\n",
    "\n",
    "# Alle Adressen als Punkte\n",
    "adress_layer = folium.FeatureGroup(name=\"Wohnadressen\")\n",
    "for _, row in gdf_main.iterrows():\n",
    "    if pd.notna(row[\"lat\"]) and pd.notna(row[\"lon\"]):\n",
    "        strasse = s(row.get(STRASSENNAME))\n",
    "        hsnr = s(row.get(HAUSNUMMER))\n",
    "        hsnrzus = s(row.get(HAUSNUMMERZUSATZ))\n",
    "\n",
    "        tooltip = strasse + \" \" + hsnr\n",
    "\n",
    "        adressen_map = folium.CircleMarker(\n",
    "            location=[row[\"lat\"], row[\"lon\"]],\n",
    "            radius=3,\n",
    "            color=\"lightgray\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "            tooltip=tooltip\n",
    "        ).add_to(adress_layer)\n",
    "adress_layer.add_to(m)\n",
    "\n",
    "def style_function(feature):\n",
    "    value = feature[\"properties\"][value_column]\n",
    "    return {\n",
    "        \"fillColor\": colormap(value),\n",
    "        \"color\": \"black\",\n",
    "        \"weight\": 0.3,\n",
    "        \"fillOpacity\": 0.2\n",
    "    }\n",
    "\n",
    "laerm_layer = folium.FeatureGroup(name=\"Lärmkarte (LDEN 2022)\")\n",
    "folium.GeoJson(\n",
    "    gdf_laerm_karte,\n",
    "    style_function=style_function,\n",
    ").add_to(laerm_layer)\n",
    "laerm_layer.add_to(m)\n",
    "colormap.add_to(m)\n",
    "\n",
    "# Schaltbare Layer\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "del(gdf_laerm_karte)  # Speicher freigeben\n",
    "\n",
    "m\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af7678d3d6d93e1d",
   "metadata": {},
   "source": [
    "## Faktoren\n",
    "Hier werden z-Werte zu alle **Einflussfaktoren** gebildet, um die Abweichung einer Ausprägung vom Standard zu erfassen. Weiterhin werden die **Gewichte** festgelegt, mit denen die Faktoren in die Wohnlagenbewertung eingehen."
   ]
  },
  {
   "cell_type": "code",
   "id": "d1fb4551bdecba3",
   "metadata": {},
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "gdf = gdf_main # zum vereinfachten Umgang\n",
    "\n",
    "# Nur Zeilen mit vollständigen Daten verwenden\n",
    "score_vars = ([\"center_distance\"] +\n",
    "              haltestellen_attribute +\n",
    "              headway_attribute +\n",
    "              einzelhandel_attribute +\n",
    "              laerm_attribute +\n",
    "              kitas_attribute +\n",
    "              grundschulen_attribute\n",
    "              )\n",
    "mask_all = gdf[score_vars].notna().all(axis=1)\n",
    "\n",
    "# Z‑Scores, fehlende Werte bleiben NaN\n",
    "gdf.loc[mask_all, \"z_centrality\"]    = -zscore(gdf.loc[mask_all, \"center_distance\"])\n",
    "gdf.loc[mask_all, \"z_shop_distance\"] = -zscore(gdf.loc[mask_all, \"shop_min_m\"])\n",
    "gdf.loc[mask_all, \"z_shop_near_500\"] =  zscore(gdf.loc[mask_all, \"shops_500m_ct\"])\n",
    "gdf.loc[mask_all, \"z_shop_near_800\"] =  zscore(gdf.loc[mask_all, \"shops_800m_ct\"])\n",
    "gdf.loc[mask_all, \"z_laerm_index_tag\"] = -zscore(gdf.loc[mask_all, \"Laerm_index_tag\"])\n",
    "gdf.loc[mask_all, \"z_kita_distance\"] = -zscore(gdf.loc[mask_all, \"kitas_min_distance_m\"])\n",
    "gdf.loc[mask_all, \"z_kita_near_500\"] =  zscore(gdf.loc[mask_all, \"kitas_count_within_500m\"])\n",
    "gdf.loc[mask_all, \"z_kita_near_800\"] =  zscore(gdf.loc[mask_all, \"kitas_count_within_800m\"])\n",
    "gdf.loc[mask_all, \"z_kita_near_1000\"] =  zscore(gdf.loc[mask_all, \"kitas_count_within_1000m\"])\n",
    "gdf.loc[mask_all, \"z_grundschul_distance\"] = -zscore(gdf.loc[mask_all, \"grundschulen_min_distance_m\"])\n",
    "gdf.loc[mask_all, \"z_grundschulen_near_500\"] =  zscore(gdf.loc[mask_all, \"grundschulen_count_within_500m\"])\n",
    "gdf.loc[mask_all, \"z_grundschulen_near_800\"] =  zscore(gdf.loc[mask_all, \"grundschulen_count_within_800m\"])\n",
    "gdf.loc[mask_all, \"z_grundschulen_near_1000\"] =  zscore(gdf.loc[mask_all, \"grundschulen_count_within_1000m\"])\n",
    "gdf.loc[mask_all, \"z_haltestelle_distance\"] = -zscore(gdf.loc[mask_all, \"haltestellen_min_distance_m\"])\n",
    "gdf.loc[mask_all, \"z_headway_score\"] = -zscore(gdf.loc[mask_all, \"headway_avg\"])\n",
    "\n",
    "# Score-Zusammenfassung nur bei vollständigen Daten\n",
    "mm_central_score_vars = [\"center_distance\", \"shop_min_m\", \"shops_500m_ct\", \"shops_800m_ct\", \"Laerm_index_tag\"]\n",
    "mask_mm_central = gdf[mm_central_score_vars].notna().all(axis=1)\n",
    "\n",
    "gdf.loc[mask_mm_central, \"score_central\"] = (\n",
    "    0.5 * gdf.loc[mask_mm_central, \"center_distance\"] +\n",
    "    0.4 * (\n",
    "        0.4 * gdf.loc[mask_mm_central, \"shop_min_m\"] +\n",
    "        0.3 * gdf.loc[mask_mm_central, \"shops_500m_ct\"] +\n",
    "        0.3 * gdf.loc[mask_mm_central, \"shops_800m_ct\"]\n",
    "    ) +\n",
    "    0.1 * gdf.loc[mask_mm_central, \"Laerm_index_tag\"]\n",
    ")\n",
    "\n",
    "# Kitas\n",
    "# Anders als kita_attribute (kein \"geometry\", was bei der Maskierung leere Zeilen von geometry rausschmeißen würde)\n",
    "mm_kita_score_vars = [\n",
    "    \"kitas_min_distance_m\",\n",
    "    \"kitas_count_within_500m\",\n",
    "    \"kitas_count_within_800m\",\n",
    "    \"kitas_count_within_1000m\"\n",
    "]\n",
    "mask_mm_kita = gdf[mm_kita_score_vars].notna().all(axis=1)\n",
    "gdf.loc[mask_mm_kita, \"score_kita\"] = (\n",
    "    0.5 * gdf.loc[mask_mm_kita, \"kitas_min_distance_m\"] +\n",
    "    0.2 * gdf.loc[mask_mm_kita, \"kitas_count_within_500m\"] +\n",
    "    0.2 * gdf.loc[mask_mm_kita, \"kitas_count_within_800m\"] +\n",
    "    0.1 * gdf.loc[mask_mm_kita, \"kitas_count_within_1000m\"]\n",
    ")\n",
    "\n",
    "# Grundschulen\n",
    "mm_grundschulen_score_vars = [\n",
    "    \"grundschulen_min_distance_m\",\n",
    "    \"grundschulen_count_within_500m\",\n",
    "    \"grundschulen_count_within_800m\",\n",
    "    \"grundschulen_count_within_1000m\"\n",
    "]\n",
    "mask_mm_grundschule = gdf[mm_grundschulen_score_vars].notna().all(axis=1)\n",
    "gdf.loc[mask_mm_grundschule, \"score_grundschule\"] = (\n",
    "    0.5 * gdf.loc[mask_mm_grundschule, \"grundschulen_min_distance_m\"] +\n",
    "    0.2 * gdf.loc[mask_mm_grundschule, \"grundschulen_count_within_500m\"] +\n",
    "    0.2 * gdf.loc[mask_mm_grundschule, \"grundschulen_count_within_800m\"] +\n",
    "    0.1 * gdf.loc[mask_mm_grundschule, \"grundschulen_count_within_1000m\"]\n",
    ")\n",
    "\n",
    "# Gesamt-Score (falls alle Teil-Scores vorhanden)\n",
    "score_all_vars = [\"score_central\", \"score_kita\", \"score_grundschule\", \"z_haltestelle_distance\", \"z_headway_score\"]\n",
    "mask_all_scores = gdf[score_all_vars].notna().all(axis=1)\n",
    "\n",
    "# Skalierte Kombination (je niedriger desto besser)\n",
    "gdf.loc[mask_all_scores, \"score_total\"] = (\n",
    "    0.4 * gdf.loc[mask_all_scores, \"score_central\"] +\n",
    "    0.15 * gdf.loc[mask_all_scores, \"score_kita\"] +\n",
    "    0.15 * gdf.loc[mask_all_scores, \"score_grundschule\"] +\n",
    "    0.15 * gdf.loc[mask_all_scores, \"z_haltestelle_distance\"] +\n",
    "    0.15 * gdf.loc[mask_all_scores, \"z_headway_score\"]\n",
    ")\n",
    "\n",
    "print(\"Anzahl gültiger Gesamt-Scores:\", gdf[\"score_total\"].notna().sum())\n",
    "print(gdf[score_all_vars].notna().sum().sort_values())\n",
    "\n",
    "all_input_vars = mm_central_score_vars + mm_kita_score_vars + mm_grundschulen_score_vars\n",
    "missing_counts = gdf[all_input_vars].isna().sum().sort_values(ascending=False)\n",
    "print(\"\")\n",
    "print(missing_counts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b801c93497168622",
   "metadata": {},
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z_vars = [\n",
    "    \"z_centrality\",\n",
    "    \"z_shop_distance\", \"z_shop_near_500\", \"z_shop_near_800\",\n",
    "    \"z_laerm_index_tag\",\n",
    "    \"z_kita_distance\", \"z_kita_near_500\", \"z_kita_near_800\", \"z_kita_near_1000\",\n",
    "    \"z_grundschul_distance\", \"z_grundschulen_near_500\", \"z_grundschulen_near_800\", \"z_grundschulen_near_1000\",\n",
    "    \"z_haltestelle_distance\", \"z_headway_score\",\n",
    "]\n",
    "X = gdf[z_vars].dropna().values\n",
    "\n",
    "inertia = []\n",
    "cluster_range = range(2, 11)  # Du kannst bis 15 hochgehen\n",
    "\n",
    "for k in cluster_range:\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(X)\n",
    "    inertia.append(model.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(cluster_range, inertia, marker='o')\n",
    "plt.title(\"Elbow-Methode: KMeans-Inertia vs. Clusteranzahl\")\n",
    "plt.xlabel(\"Anzahl Cluster (k)\")\n",
    "plt.ylabel(\"Inertia (Distanz innerhalb der Cluster)\")\n",
    "plt.xticks(cluster_range)\n",
    "plt.grid(True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "399ea032fbc29c17",
   "metadata": {},
   "source": [
    "# Silhouetten-Analyse\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouettes = []\n",
    "\n",
    "for k in cluster_range:\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = model.fit_predict(X)\n",
    "    score = silhouette_score(X, labels, random_state=42)\n",
    "    silhouettes.append(score)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(cluster_range, silhouettes, marker='o', color=\"green\")\n",
    "plt.title(\"Silhouetten-Score pro Clusteranzahl\")\n",
    "plt.xlabel(\"Anzahl Cluster (k)\")\n",
    "plt.ylabel(\"Durchschn. Silhouetten-Koeffizient\")\n",
    "plt.xticks(cluster_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5356ceff36097c79",
   "metadata": {},
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = gdf[z_vars].dropna().values  # Nur vollständige Zeilen\n",
    "\n",
    "model = KMeans(n_clusters=5, random_state=42).fit(X)\n",
    "\n",
    "mask_complete = gdf[z_vars].notna().all(axis=1)\n",
    "gdf.loc[mask_complete, \"cluster\"] = model.labels_\n",
    "\n",
    "cluster_centers = pd.DataFrame(model.cluster_centers_, columns=z_vars)\n",
    "cluster_centers.index.name = \"Cluster\"\n",
    "print(cluster_centers.round(2))\n",
    "\n",
    "\n",
    "# Visualisierung mit Dimension Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "X_scaled = X  # bereits Z-Scores → keine erneute Skalierung nötig\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=model.labels_, cmap=\"tab10\", alpha=0.6)\n",
    "plt.title(\"KMeans-Cluster (PCA 2D-Projektion)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6097678054722e03",
   "metadata": {},
   "source": [
    "import pandas as pd, folium\n",
    "\n",
    "gdf = gdf[gdf[\"lat\"].notna() & gdf[\"lon\"].notna() & gdf[\"cluster\"].notna()]\n",
    "gdf[\"cluster\"] = gdf[\"cluster\"].astype(int)\n",
    "\n",
    "# Farbpalette für 5 Cluster\n",
    "cluster_colors = {\n",
    "    0: \"#e41a1c\",   # Cluster 0 - rot\n",
    "    1: \"#377eb8\",   # Cluster 1 - blau\n",
    "    2: \"#4daf4a\",   # Cluster 2 - grün\n",
    "    3: \"#984ea3\",   # Cluster 3 - lila\n",
    "    4: \"#ff7f00\",   # Cluster 4 - orange\n",
    "    5: \"#666666\",\n",
    "    6: \"#a65628\",\n",
    "    7: \"#66cd2c\",\n",
    "    # Füge weitere hinzu falls nötig!\n",
    "}\n",
    "\n",
    "# Farben für Wohnlagen (grün - gelb)\n",
    "color_map = {\n",
    "    \"1 Top\":      \"#fee08b\",\n",
    "    \"2\":          \"#d9ef8b\",\n",
    "    \"3\":          \"#a6d96a\",\n",
    "    \"4\":          \"#66bd63\",\n",
    "    \"5 schwach\":  \"#1a9850\",\n",
    "}\n",
    "\n",
    "m = folium.Map(location=CITY_CENTER, zoom_start=13, tiles=\"cartodbpositron\")\n",
    "\n",
    "# Add layer markers\n",
    "add_markers_from_csv(map_obj=m, csv_path=\"out/einzelhandel_geocoded.csv\", color=\"blue\", icon=\"shopping-cart\", layer_name=\"Einzelhandel\")\n",
    "add_markers_from_csv(map_obj=m, csv_path=\"out/grundschulen_geocoded.csv\", color=\"green\", icon=\"graduation-cap\", layer_name=\"Grundschulen\")\n",
    "add_markers_from_csv(map_obj=m, csv_path=\"out/kitas_geocoded.csv\", color=\"beige\", icon=\"child\", layer_name=\"Kitas\")\n",
    "#add_markers_from_csv(map_obj=m,csv_path=\"data/haltestellen_geocoded.csv\",color=\"lightgray\", icon=\"bus\", layer_name=\"Haltestellen\")\n",
    "\n",
    "valid_kita_json = gdf_main[\"kitas_geometry\"].apply(lambda x: isinstance(x, str))\n",
    "print(\"Gültige JSON-Einträge:\", valid_kita_json.sum(), \"/\", len(gdf_main))\n",
    "\n",
    "for _, r in gdf.iterrows():\n",
    "    c = cluster_colors.get(r.cluster, \"#666666\")\n",
    "    folium.CircleMarker(\n",
    "        location=[r.lat, r.lon],\n",
    "        radius=3,\n",
    "        color=c,\n",
    "        fill=True,\n",
    "        fill_color=c,\n",
    "        fill_opacity=0.85,\n",
    "        tooltip=f\"{r.Straßenname} {r.Hsnr} – Cluster {r.cluster}\"\n",
    "    ).add_to(m)\n",
    "\n",
    "# Optional: Layer Control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "print(gdf.shape)\n",
    "\n",
    "#m.save(\"wohnlagen_clusterkarte.html\")\n",
    "m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4682611337ac7fa7",
   "metadata": {},
   "source": [
    "# Validierung"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a006e046a8d03a",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "zscore_cols = [\n",
    "    \"z_centrality\",\n",
    "    \"z_shop_distance\",\n",
    "    \"z_laerm_index_tag\",\n",
    "    \"z_kita_distance\",\n",
    "    \"z_grundschul_distance\",\n",
    "    \"z_haltestelle_distance\",\n",
    "    \"z_headway_score\",\n",
    "    \"z_haltestellen_count_500\"\n",
    "]\n",
    "\n",
    "# Nur Spalten verwenden, die im gdf enthalten sind\n",
    "cols = [col for col in zscore_cols if col in gdf.columns]\n",
    "\n",
    "# Korrelationsmatrix berechnen\n",
    "corr = gdf[cols].corr()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap=\"vlag\", center=0)\n",
    "plt.title(\"Korrelationsmatrix der Z-Scores\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9fae0eebad9d76ad",
   "metadata": {},
   "source": [
    "## Interaktive Karte für Bewertung einzelner Adressen"
   ]
  },
  {
   "cell_type": "code",
   "id": "e551b9ad6731ac78",
   "metadata": {},
   "source": [
    "import folium\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Widget für Adresseingabe\n",
    "text_input = widgets.Text(\n",
    "    value='Immenweg 56',\n",
    "    placeholder='Straßenname Hausnummer',\n",
    "    description='Adresse:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Button\n",
    "button = widgets.Button(description=\"Zeige Routen\")\n",
    "\n",
    "# Ausgabe-Bereich für die Karte\n",
    "output = widgets.Output()\n",
    "\n",
    "# Funktion zum Einfügen einer Route + Zielmarker\n",
    "def add_route(m, geojson_str, color, label, icon, distance=None):\n",
    "    if isinstance(geojson_str, str):\n",
    "        try:\n",
    "            geo = json.loads(geojson_str)\n",
    "            if geo.get(\"type\") == \"LineString\":\n",
    "                coords = [(y, x) for x, y in geo[\"coordinates\"]]\n",
    "                distance_text = f\" – {int(distance)} m\" if distance else \"\"\n",
    "                tooltip_text = f\"{label}{distance_text}\"\n",
    "\n",
    "                folium.PolyLine(\n",
    "                    locations=coords,\n",
    "                    color=color,\n",
    "                    weight=4,\n",
    "                    opacity=0.9,\n",
    "                    tooltip=tooltip_text\n",
    "                ).add_to(m)\n",
    "\n",
    "                end = coords[-1]\n",
    "                folium.Marker(\n",
    "                    location=end,\n",
    "                    icon=folium.Icon(color=color, icon=icon, prefix=\"fa\"),\n",
    "                    tooltip=f\"Ziel: {label}{distance_text}\"\n",
    "                ).add_to(m)\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {label}: {e}\")\n",
    "\n",
    "# Funktion zum Anzeigen der Karte\n",
    "def show_routes(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        addr = text_input.value.strip().lower()\n",
    "\n",
    "        filtered = gdf_main[gdf_main[\"Adresse_merge\"].str.lower().str.contains(addr)]\n",
    "        if filtered.empty:\n",
    "            print(\"Keine Adresse gefunden.\")\n",
    "            return\n",
    "\n",
    "        row = filtered.iloc[0]\n",
    "        m = folium.Map(location=[row.lat, row.lon], zoom_start=15, tiles=\"cartodbpositron\")\n",
    "        folium.Marker(location=[row.lat, row.lon], tooltip=\"Adresse\").add_to(m)\n",
    "\n",
    "        # ▸ Routen einfügen\n",
    "        add_route(m, row.get(\"kitas_geometry\"), \"orange\", \"Nächste Kita\", \"child\", row.get(\"kitas_min_distance_m\"))\n",
    "        add_route(m, row.get(\"grundschulen_geometry\"), \"green\", \"Nächste Grundschule\", \"graduation-cap\", row.get(\"grundschulen_min_distance_m\"))\n",
    "        add_route(m, row.get(\"center_route\"), \"lightgray\", \"Weg zum Zentrum\", \"arrows-to-circle\", row.get(\"center_distance\"))\n",
    "        add_route(m, row.get(\"haltestellen_geometry\"), \"gray\", \"Nächste Haltestelle\", \"bus\", row.get(\"haltestellen_min_distance_m\"))\n",
    "\n",
    "        display(m)\n",
    "\n",
    "# Button-Event\n",
    "button.on_click(show_routes)\n",
    "\n",
    "# UI anzeigen\n",
    "display(text_input, button, output)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WohnlagenBRB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
